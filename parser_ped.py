import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin


class PedagogyParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ pedsovet.org —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BeautifulSoup
    """
    
    def __init__(self):
        self.base_url = "https://pedsovet.org"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self):
        """–ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {self.base_url}")
        
        try:
            response = self.session.get(self.base_url)
            response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            print("‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return soup
            
        except requests.exceptions.RequestException as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return None
    
    def find_all_elements_demo(self, soup):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∏–∑ –∑–∞–¥–∞–Ω–∏—è"""
        print("\n" + "="*60)
        print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–¢–û–î–û–í BEAUTIFULSOUP:")
        print("="*60)
        
        # 1. –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥—É
        print("\n1. –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–≥—É <a> (–≤—Å–µ —Å—Å—ã–ª–∫–∏):")
        all_links = soup.find_all('a')
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(all_links)}")
        if all_links[:3]:  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 3
            for i, link in enumerate(all_links[:3], 1):
                text = link.text.strip()[:50] + "..." if len(link.text.strip()) > 50 else link.text.strip()
                print(f"   {i}. –¢–µ–∫—Å—Ç: {text}")
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å—É
        print("\n2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å—É 'article':")
        articles_by_class = soup.find_all(class_='article')
        print(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(articles_by_class)}")
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ id
        print("\n3. –ü–æ–∏—Å–∫ –ø–æ id (–ª—é–±–æ–º—É):")
        # –ò—â–µ–º –ª—é–±–æ–π —ç–ª–µ–º–µ–Ω—Ç —Å id
        element_with_id = soup.find(id=True)
        if element_with_id:
            id_name = list(element_with_id.attrs.get('id', []))[0] if isinstance(element_with_id.attrs.get('id', []), list) else element_with_id.attrs.get('id', '')
            print(f"   –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç —Å id='{id_name}'")
        else:
            print("   –≠–ª–µ–º–µ–Ω—Ç—ã —Å id –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # 4. –ü–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç—É
        print("\n4. –ü–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç—É 'href' (—Å—Å—ã–ª–∫–∏):")
        links_with_href = soup.find_all(href=True)
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º href: {len(links_with_href)}")
        
        # 5. CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        print("\n5. –ü–æ–∏—Å–∫ —Å –ø–æ–º–æ—â—å—é CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤:")
        # –í—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = soup.select('h1, h2, h3, h4')
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (h1-h4): {len(headers)}")
        
        return True
    
    def parse_articles(self, soup):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π"""
        print("\n" + "="*60)
        print("–ü–ê–†–°–ò–ù–ì –°–¢–ê–¢–ï–ô:")
        print("="*60)
        
        articles = []
        
        # –°–ü–û–°–û–ë 1: –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (–≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ —Å—Å—ã–ª–∫–æ–π)
        print("–°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (div, article, section —Å —Å—Å—ã–ª–∫–∞–º–∏)")
        
        # –ò—â–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        containers = soup.find_all(['div', 'article', 'section'])
        
        for container in containers:
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            link_tag = container.find('a')
            
            if link_tag and link_tag.get('href'):
                # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (h1-h4) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                title_tag = container.find(['h1', 'h2', 'h3', 'h4'])
                
                if title_tag:
                    title = title_tag.text.strip()
                else:
                    title = link_tag.text.strip()
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                link = link_tag.get('href')
                
                # –î–µ–ª–∞–µ–º —Å—Å—ã–ª–∫—É –∞–±—Å–æ–ª—é—Ç–Ω–æ–π
                if link:
                    link = urljoin(self.base_url, link)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å—Ç–∞—Ç—å—é (–µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞)
                if title and len(title) > 10 and link:
                    article_data = {
                        'title': title,
                        'link': link
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                    if article_data not in articles:
                        articles.append(article_data)
        
        # –°–ü–û–°–û–ë 2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º)
        print("\n–°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º (—ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ card, item, post)")
        
        card_classes = ['card', 'item', 'post', 'news', 'article', 'material']
        
        for card_class in card_classes:
            cards = soup.find_all(class_=card_class)
            
            for card in cards:
                link_tag = card.find('a')
                if link_tag and link_tag.get('href'):
                    title = link_tag.text.strip()
                    if not title or len(title) < 10:
                        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –¥—Ä—É–≥–∏—Ö —Ç–µ–≥–∞—Ö
                        header = card.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                        if header:
                            title = header.text.strip()
                    
                    link = urljoin(self.base_url, link_tag.get('href'))
                    
                    if title and len(title) > 10 and link:
                        article_data = {
                            'title': title,
                            'link': link
                        }
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ —Å—Å—ã–ª–∫–µ
                        is_duplicate = False
                        for existing_article in articles:
                            if (existing_article['title'] == title or 
                                existing_article['link'] == link):
                                is_duplicate = True
                                break
                        
                        if not is_duplicate:
                            articles.append(article_data)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
        articles = articles[:15]
        
        print(f"\n–ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        return articles
    
    def display_results(self, articles):
        """–í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        print("\n" + "="*80)
        print("–ù–ê–ô–î–ï–ù–ù–´–ï –°–¢–ê–¢–¨–ò:")
        print("="*80)
        
        for i, article in enumerate(articles, 1):
            print(f"{i:2}. {article['title'][:70]}{'...' if len(article['title']) > 70 else ''}")
            print(f"    –°—Å—ã–ª–∫–∞: {article['link']}")
            print()
    
    def save_to_json(self, articles, filename="articles.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
            return True
        except Exception as e:
            print(f"\n‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            return False
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
        print("="*60)
        print("–ü–ê–†–°–ï–† –î–õ–Ø PEDSOVET.ORG (BeautifulSoup)")
        print("="*60)
        
        # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        soup = self.get_page()
        if not soup:
            return []
        
        # 2. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∏–∑ –∑–∞–¥–∞–Ω–∏—è
        self.find_all_elements_demo(soup)
        
        # 3. –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏
        articles = self.parse_articles(soup)
        
        # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if articles:
            self.display_results(articles)
            
            # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self.save_to_json(articles)
            
            print(f"\n{'='*60}")
            print(f"–ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù! –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
            print(f"{'='*60}")
        else:
            print("\n‚úó –°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã.")
        
        return articles


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = PedagogyParser()
    articles = parser.run()
    
    # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if articles:
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        print(f"   ‚Ä¢ –ü—Ä–∏–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞: {articles[0]['title'][:50]}...")
        print(f"   ‚Ä¢ –ü—Ä–∏–º–µ—Ä —Å—Å—ã–ª–∫–∏: {articles[0]['link'][:60]}...")


if __name__ == "__main__":
    main()